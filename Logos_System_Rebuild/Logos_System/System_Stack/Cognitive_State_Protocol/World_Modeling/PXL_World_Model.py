# HEADER_TYPE: CANONICAL_REBUILD_MODULE
# EXECUTION: CONTROLLED
# AUTHORITY: GOVERNED
# ORIGIN: SYSTEMATIC_REWRITE

"""
MODAL VECTOR SPACE WORLD MODEL (MVS-WM)
A maximally robust ontological-epistemic world modeling system
Designed for integration with PXL-based reasoning engines
"""

from typing import Dict, List, Optional, Set, Tuple, Any, Union
from dataclasses import dataclass, field
from enum import Enum, auto
import numpy as np
import json
import hashlib
from datetime import datetime, timezone
from pathlib import Path
import pickle
import zlib
from scipy import spatial
from scipy import stats
import networkx as nx
from collections import defaultdict, deque
import asyncio
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
import heapq

# ==================== CORE MODAL PRIMITIVES ====================

class ModalDimension(Enum):
    """Fundamental modal dimensions in the vector space"""
    NECESSITY = auto()           # â–¡ (necessity)
    POSSIBILITY = auto()         # â—‡ (possibility)
    ACTUALITY = auto()           # @ (actuality)
    PROBABILITY = auto()         # Pr (probability)
    NORMATIVITY = auto()         # Ought (normative force)
    TEMPORALITY = auto()         # Temporal modality
    EPISTEMIC = auto()           # K (knowledge)
    DOXASTIC = auto()            # B (belief)
    INTENTIONAL = auto()         # I (intention)
    CAUSAL = auto()              # C (causation)
    TELEOLOGICAL = auto()        # T (teleology)
    AESTHETIC = auto()          # A (aesthetic value)
    ETHICAL = auto()            # E (ethical value)
    EXISTENTIAL = auto()        # âˆƒ (existential weight)
    COHERENCE = auto()          # PXL coherence
    
    # PXL Triune Dimensions
    PXL_IDENTITY = auto()       # ð•€â‚ grounding
    PXL_CONTRADICTION = auto()  # ð•€â‚‚ grounding
    PXL_EXCLUDED_MIDDLE = auto() # ð•€â‚ƒ grounding

@dataclass
class ModalVector:
    """A point in modal vector space"""
    coordinates: np.ndarray  # Shape: (n_dimensions,)
    dimensions: List[ModalDimension]
    certainty: np.ndarray    # Certainty for each dimension (0-1)
    timestamp: datetime
    source: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        if len(self.coordinates) != len(self.dimensions):
            raise ValueError("Coordinates must match dimensions length")
        if len(self.certainty) != len(self.dimensions):
            raise ValueError("Certainty must match dimensions length")
    
    @property
    def id(self) -> str:
        """Unique identifier based on vector state"""
        vector_hash = hashlib.sha256(
            self.coordinates.tobytes() + 
            str([d.value for d in self.dimensions]).encode() +
            self.timestamp.isoformat().encode()
        ).hexdigest()[:16]
        return f"mvs:{vector_hash}"
    
    def similarity(self, other: 'ModalVector', 
                  weights: Optional[np.ndarray] = None) -> float:
        """Compute similarity between modal vectors"""
        if not self._compatible_dimensions(other):
            return 0.0
        
        # Align dimensions
        aligned_self, aligned_other = self._align_dimensions(other)
        
        # Weighted cosine similarity
        if weights is None:
            weights = np.ones(len(aligned_self))
        
        # Consider certainty in similarity
        certainty_weights = (self.certainty + other.certainty) / 2
        
        similarity = 1 - spatial.distance.cosine(
            aligned_self * certainty_weights,
            aligned_other * certainty_weights * weights
        )
        
        return max(0.0, similarity)  # Ensure non-negative
    
    def modal_distance(self, other: 'ModalVector',
                      dimension: Optional[ModalDimension] = None) -> float:
        """Compute modal distance in specific dimension(s)"""
        if dimension:
            if dimension not in self.dimensions or dimension not in other.dimensions:
                return float('inf')
            
            idx_self = self.dimensions.index(dimension)
            idx_other = other.dimensions.index(dimension)
            
            return abs(self.coordinates[idx_self] - other.coordinates[idx_other])
        else:
            # Overall modal distance
            aligned_self, aligned_other = self._align_dimensions(other)
            return np.linalg.norm(aligned_self - aligned_other)
    
    def project_to_pxl(self) -> Dict[str, float]:
        """Project modal vector to PXL triune dimensions"""
        projection = {}
        
        # Map to PXL dimensions if present
        for pxl_dim in [ModalDimension.PXL_IDENTITY, 
                       ModalDimension.PXL_CONTRADICTION,
                       ModalDimension.PXL_EXCLUDED_MIDDLE]:
            if pxl_dim in self.dimensions:
                idx = self.dimensions.index(pxl_dim)
                projection[pxl_dim.name] = {
                    'value': float(self.coordinates[idx]),
                    'certainty': float(self.certainty[idx])
                }
        
        # Derive PXL properties from other dimensions
        if ModalDimension.COHERENCE in self.dimensions:
            idx = self.dimensions.index(ModalDimension.COHERENCE)
            projection['coherence_score'] = float(self.coordinates[idx])
        
        # Compute triune balance
        triune_dims = [ModalDimension.NECESSITY, 
                      ModalDimension.POSSIBILITY,
                      ModalDimension.ACTUALITY]
        triune_values = []
        for dim in triune_dims:
            if dim in self.dimensions:
                idx = self.dimensions.index(dim)
                triune_values.append(self.coordinates[idx])
        
        if triune_values:
            projection['triune_balance'] = float(np.std(triune_values))
        
        return projection

# ==================== FRACTAL EMBEDDING SYSTEM ====================

@dataclass
class FractalEmbedding:
    """Fractal embedding of modal vectors across scales"""
    base_vector: ModalVector
    scales: List[float]  # Fractal scales (e.g., [1.0, 0.5, 0.25, ...])
    embeddings: Dict[float, np.ndarray]  # Scale -> embedding
    invariants: List[str]  # Scale-invariant properties
    dimension: float  # Fractal dimension estimate
    
    def __post_init__(self):
        self._validate_scales()
    
    def _validate_scales(self):
        """Ensure scales are positive and decreasing"""
        if not self.scales:
            raise ValueError("At least one scale required")
        if any(s <= 0 for s in self.scales):
            raise ValueError("Scales must be positive")
        if sorted(self.scales, reverse=True) != self.scales:
            raise ValueError("Scales should be decreasing")
    
    def embed_at_scale(self, scale: float) -> np.ndarray:
        """Get embedding at specific fractal scale"""
        if scale not in self.embeddings:
            # Interpolate if scale not present
            closest = min(self.embeddings.keys(), 
                         key=lambda s: abs(s - scale))
            if abs(closest - scale) < 0.01:
                return self.embeddings[closest]
            else:
                # Create new embedding through fractal transformation
                return self._generate_fractal_embedding(scale)
        return self.embeddings[scale]
    
    def _generate_fractal_embedding(self, scale: float) -> np.ndarray:
        """Generate new embedding through fractal transformation"""
        # Find two nearest scales
        scales = sorted(self.embeddings.keys())
        lower = max(s for s in scales if s <= scale) if any(s <= scale for s in scales) else scales[0]
        upper = min(s for s in scales if s >= scale) if any(s >= scale for s in scales) else scales[-1]
        
        # Logarithmic interpolation for fractal scaling
        if lower == upper:
            return self.embeddings[lower]
        
        ratio = (np.log(scale) - np.log(lower)) / (np.log(upper) - np.log(lower))
        lower_embed = self.embeddings[lower]
        upper_embed = self.embeddings[upper]
        
        # Fractal interpolation with self-similarity preservation
        interpolated = lower_embed * (1 - ratio) + upper_embed * ratio
        
        # Add fractal noise proportional to scale
        if scale < 1.0:
            noise_magnitude = 0.1 * (1 - scale) * np.random.randn(*lower_embed.shape)
            interpolated += noise_magnitude
        
        return interpolated
    
    def compute_fractal_dimension(self) -> float:
        """Compute fractal dimension from embeddings"""
        if len(self.scales) < 2:
            return 1.0
        
        # Use box-counting method on embeddings
        scales_log = np.log(self.scales)
        measures = []
        
        for scale in self.scales:
            embedding = self.embeddings[scale]
            # Simplified box count
            unique_boxes = len(np.unique(np.round(embedding * 10)))
            measures.append(unique_boxes)
        
        measures_log = np.log(measures)
        
        # Linear regression for fractal dimension
        if len(scales_log) > 1:
            slope, _, _, _, _ = stats.linregress(scales_log, measures_log)
            return abs(slope)
        
        return 1.0

# ==================== WORLD STATE REPRESENTATION ====================

@dataclass
class WorldState:
    """Complete world state at a moment"""
    timestamp: datetime
    modal_vectors: Dict[str, ModalVector]  # entity_id -> modal vector
    relations: List[Tuple[str, str, str, float]]  # (source, relation, target, strength)
    global_modality: np.ndarray  # Global modal field
    consistency_score: float
    entropy: float
    pxl_grounding: Optional[Dict] = None
    
    @property
    def state_hash(self) -> str:
        """Compute cryptographic hash of world state"""
        state_data = {
            'timestamp': self.timestamp.isoformat(),
            'vectors': {k: v.id for k, v in self.modal_vectors.items()},
            'relations': self.relations,
            'global_modality': self.global_modality.tolist(),
            'consistency': self.consistency_score
        }
        
        state_json = json.dumps(state_data, sort_keys=True, separators=(',', ':'))
        return f"world_state:{hashlib.sha256(state_json.encode()).hexdigest()[:32]}"
    
    def get_entity_modality(self, entity_id: str, 
                           dimension: ModalDimension) -> Optional[float]:
        """Get modality value for specific entity and dimension"""
        if entity_id not in self.modal_vectors:
            return None
        
        vector = self.modal_vectors[entity_id]
        if dimension not in vector.dimensions:
            return None
        
        idx = vector.dimensions.index(dimension)
        return float(vector.coordinates[idx])
    
    def compute_modal_field(self, position: np.ndarray) -> np.ndarray:
        """Compute modal field value at given position"""
        # This is where we'd implement modal field computations
        # For now, return weighted sum of nearby entities
        if not self.modal_vectors:
            return np.zeros(len(ModalDimension))
        
        # Simple implementation: average of all vectors
        all_vectors = list(self.modal_vectors.values())
        if not all_vectors:
            return np.zeros(len(ModalDimension))
        
        # Align all vectors to same dimensions
        base_dims = all_vectors[0].dimensions
        aligned_vectors = []
        
        for vec in all_vectors:
            aligned, _ = vec._align_dimensions(all_vectors[0])
            aligned_vectors.append(aligned)
        
        return np.mean(aligned_vectors, axis=0)

# ==================== DYNAMIC WORLD MODEL ====================

class DynamicWorldModel:
    """
    Core world modeling engine with temporal dynamics,
    modal consistency, and fractal embeddings
    """
    
    def __init__(self, 
                 initial_dimensions: Optional[List[ModalDimension]] = None,
                 cache_size: int = 10000):
        
        # Core dimensions
        self.dimensions = initial_dimensions or list(ModalDimension)
        self.dimension_indices = {dim: i for i, dim in enumerate(self.dimensions)}
        
        # State tracking
        self.world_states: List[WorldState] = []
        self.entity_history: Dict[str, List[Tuple[datetime, ModalVector]]] = {}
        
        # Fractal embeddings
        self.fractal_embeddings: Dict[str, FractalEmbedding] = {}
        
        # Modal consistency engine
        self.consistency_graph = nx.Graph()
        self.modal_constraints: List[Dict] = []
        
        # Caching for performance
        self._similarity_cache = {}
        self._projection_cache = {}
        self.cache_size = cache_size
        
        # Metrics
        self.metrics = {
            'updates': 0,
            'consistency_violations': 0,
            'fractal_embeddings_created': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        # Initialize with empty world state
        self.current_state = self._create_initial_state()
    
    def _create_initial_state(self) -> WorldState:
        """Create initial empty world state"""
        return WorldState(
            timestamp=datetime.now(timezone.utc),
            modal_vectors={},
            relations=[],
            global_modality=np.zeros(len(self.dimensions)),
            consistency_score=1.0,
            entropy=0.0
        )
    
    def update_entity(self, 
                     entity_id: str,
                     vector: ModalVector,
                     source: str = "unknown",
                     validate_consistency: bool = True) -> Dict:
        """
        Update or add an entity to the world model
        Returns update results with validation
        """
        
        # Validate vector dimensions
        if not set(vector.dimensions).issubset(set(self.dimensions)):
            # Add missing dimensions with neutral values
            vector = self._expand_vector_dimensions(vector)
        
        # Check modal consistency
        consistency_check = {}
        if validate_consistency:
            consistency_check = self._check_modal_consistency(entity_id, vector)
            
            if not consistency_check.get('consistent', True):
                self.metrics['consistency_violations'] += 1
                
                # Apply consistency resolution
                if consistency_check.get('resolvable', False):
                    vector = self._resolve_consistency_violation(vector, 
                                                               consistency_check)
                else:
                    # Log but still update (with warning)
                    pass
        
        # Update entity history
        if entity_id not in self.entity_history:
            self.entity_history[entity_id] = []
        self.entity_history[entity_id].append((vector.timestamp, vector))
        
        # Update current state
        self.current_state.modal_vectors[entity_id] = vector
        
        # Update relations based on modal similarities
        self._update_entity_relations(entity_id, vector)
        
        # Recompute global modality
        self._recompute_global_modality()
        
        # Update fractal embedding if needed
        if len(self.entity_history[entity_id]) >= 3:
            self._update_fractal_embedding(entity_id)
        
        self.metrics['updates'] += 1
        
        return {
            'entity_id': entity_id,
            'timestamp': vector.timestamp.isoformat(),
            'consistency_check': consistency_check,
            'fractal_updated': len(self.entity_history[entity_id]) >= 3,
            'global_modality_impact': self._compute_impact(entity_id, vector)
        }
    
    def _check_modal_consistency(self, 
                               entity_id: str,
                               new_vector: ModalVector) -> Dict:
        """Check modal consistency with existing world state"""
        
        violations = []
        warnings = []
        
        if entity_id in self.current_state.modal_vectors:
            old_vector = self.current_state.modal_vectors[entity_id]
            
            # Check for radical modal shifts
            modal_distance = new_vector.modal_distance(old_vector)
            if modal_distance > 2.0:  # Threshold
                violations.append({
                    'type': 'radical_modal_shift',
                    'distance': float(modal_distance),
                    'threshold': 2.0
                })
            
            # Check PXL triune consistency
            pxl_projection = new_vector.project_to_pxl()
            if pxl_projection:
                # Ensure PXL dimensions maintain coherence
                if 'triune_balance' in pxl_projection:
                    if pxl_projection['triune_balance'] > 0.8:
                        warnings.append({
                            'type': 'high_triune_imbalance',
                            'balance': pxl_projection['triune_balance']
                        })
        
        # Check consistency with modal constraints
        for constraint in self.modal_constraints:
            if self._violates_constraint(new_vector, constraint):
                violations.append({
                    'type': 'modal_constraint_violation',
                    'constraint': constraint.get('name', 'unknown')
                })
        
        # Check temporal coherence with entity history
        if entity_id in self.entity_history and self.entity_history[entity_id]:
            last_vector = self.entity_history[entity_id][-1][1]
            temporal_coherence = self._check_temporal_coherence(last_vector, 
                                                              new_vector)
            if not temporal_coherence['coherent']:
                violations.append({
                    'type': 'temporal_incoherence',
                    'reason': temporal_coherence['reason']
                })
        
        return {
            'consistent': len(violations) == 0,
            'violations': violations,
            'warnings': warnings,
            'resolvable': len(violations) <= 2  # Arbitrary threshold
        }
    
    def _update_fractal_embedding(self, entity_id: str):
        """Update or create fractal embedding for entity"""
        
        if entity_id not in self.entity_history:
            return
        
        history = self.entity_history[entity_id]
        if len(history) < 3:
            return
        
        # Extract vectors from history
        vectors = [vec for _, vec in history[-10:]]  # Last 10 states
        
        # Create scales based on temporal distance
        scales = [1.0 / (i + 1) for i in range(len(vectors))]
        
        # Create embeddings at each scale
        embeddings = {}
        base_vector = vectors[-1]  # Most recent
        
        for i, (scale, vector) in enumerate(zip(scales, vectors)):
            # Transform vector to fractal scale
            scaled_coords = vector.coordinates * scale
            
            # Add self-similarity noise
            if scale < 1.0:
                noise = np.random.randn(*scaled_coords.shape) * 0.1 * (1 - scale)
                scaled_coords += noise
            
            embeddings[scale] = scaled_coords
        
        # Compute invariants
        invariants = self._compute_fractal_invariants(vectors)
        
        # Create or update fractal embedding
        if entity_id in self.fractal_embeddings:
            # Update existing embedding
            self.fractal_embeddings[entity_id].embeddings.update(embeddings)
            self.fractal_embeddings[entity_id].invariants.extend(invariants)
            self.fractal_embeddings[entity_id].invariants = list(set(
                self.fractal_embeddings[entity_id].invariants
            ))
        else:
            # Create new embedding
            fractal_dim = self._estimate_fractal_dimension(vectors)
            self.fractal_embeddings[entity_id] = FractalEmbedding(
                base_vector=base_vector,
                scales=scales,
                embeddings=embeddings,
                invariants=invariants,
                dimension=fractal_dim
            )
            self.metrics['fractal_embeddings_created'] += 1
    
    def query_modal_space(self,
                         position: np.ndarray,
                         radius: float = 1.0,
                         dimensions: Optional[List[ModalDimension]] = None,
                         min_similarity: float = 0.7) -> List[Dict]:
        """
        Query modal space for entities within radius
        Returns entities with similarity scores
        """
        
        if dimensions is None:
            dimensions = self.dimensions
        
        results = []
        
        for entity_id, vector in self.current_state.modal_vectors.items():
            # Align dimensions
            aligned_vec, aligned_pos = vector._align_dimensions(
                ModalVector(position, dimensions, np.ones(len(dimensions)),
                           datetime.now(), "query")
            )
            
            # Compute distance
            distance = np.linalg.norm(aligned_vec - aligned_pos)
            
            if distance <= radius:
                similarity = 1 - (distance / radius)
                
                if similarity >= min_similarity:
                    # Get fractal embedding if exists
                    fractal_info = None
                    if entity_id in self.fractal_embeddings:
                        fractal_info = {
                            'dimension': self.fractal_embeddings[entity_id].dimension,
                            'invariants': self.fractal_embeddings[entity_id].invariants[:5]
                        }
                    
                    results.append({
                        'entity_id': entity_id,
                        'similarity': float(similarity),
                        'distance': float(distance),
                        'modal_values': {d.name: float(vector.coordinates[i]) 
                                        for i, d in enumerate(vector.dimensions) 
                                        if d in dimensions},
                        'fractal_info': fractal_info,
                        'certainty': float(np.mean(vector.certainty))
                    })
        
        # Sort by similarity
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results
    
    def predict_modal_evolution(self,
                              entity_id: str,
                              steps: int = 5,
                              method: str = 'fractal') -> List[ModalVector]:
        """
        Predict future modal evolution of entity
        Uses fractal patterns for prediction
        """
        
        if entity_id not in self.entity_history:
            return []
        
        history = self.entity_history[entity_id]
        if len(history) < 3:
            return []
        
        # Extract historical vectors
        timestamps, vectors = zip(*history)
        vectors = list(vectors)
        
        predictions = []
        
        if method == 'fractal' and entity_id in self.fractal_embeddings:
            # Use fractal embedding for prediction
            fractal = self.fractal_embeddings[entity_id]
            
            for step in range(1, steps + 1):
                # Predict at smaller fractal scale (further future)
                scale = 1.0 / (step + 1)
                
                # Get embedding at scale
                embedding = fractal.embed_at_scale(scale)
                
                # Create predicted vector
                predicted_coords = embedding
                
                # Adjust certainty based on prediction distance
                certainty = np.maximum(0.1, 1.0 - (step * 0.2)) * vectors[-1].certainty
                
                predicted_vector = ModalVector(
                    coordinates=predicted_coords,
                    dimensions=vectors[-1].dimensions,
                    certainty=certainty,
                    timestamp=timestamps[-1],  # Will be updated by caller
                    source=f"prediction_{method}_step{step}",
                    metadata={'prediction_step': step, 'method': method}
                )
                
                predictions.append(predicted_vector)
        
        elif method == 'linear':
            # Simple linear extrapolation
            recent_vectors = vectors[-3:]  # Last 3 vectors
            
            if len(recent_vectors) >= 2:
                # Compute trend
                trend = recent_vectors[-1].coordinates - recent_vectors[-2].coordinates
                
                for step in range(1, steps + 1):
                    predicted_coords = recent_vectors[-1].coordinates + (trend * step)
                    
                    # Dampen certainty with prediction distance
                    certainty = np.maximum(0.1, 1.0 - (step * 0.3)) * recent_vectors[-1].certainty
                    
                    predicted_vector = ModalVector(
                        coordinates=predicted_coords,
                        dimensions=recent_vectors[-1].dimensions,
                        certainty=certainty,
                        timestamp=timestamps[-1],
                        source=f"prediction_{method}_step{step}",
                        metadata={'prediction_step': step, 'method': method}
                    )
                    
                    predictions.append(predicted_vector)
        
        return predictions
    
    def compute_world_entropy(self) -> float:
        """Compute entropy of current world state"""
        
        if not self.current_state.modal_vectors:
            return 0.0
        
        # Collect all modal values
        all_values = []
        for vector in self.current_state.modal_vectors.values():
            all_values.extend(vector.coordinates)
        
        if len(all_values) < 2:
            return 0.0
        
        # Compute histogram entropy
        hist, _ = np.histogram(all_values, bins=20, density=True)
        hist = hist[hist > 0]
        
        entropy = -np.sum(hist * np.log(hist))
        
        # Normalize by number of entities
        normalized = entropy / len(self.current_state.modal_vectors)
        
        self.current_state.entropy = float(normalized)
        return float(normalized)
    
    def add_modal_constraint(self,
                            constraint_type: str,
                            dimension: ModalDimension,
                            condition: Dict[str, Any]):
        """Add modal constraint to world model"""
        
        constraint = {
            'type': constraint_type,
            'dimension': dimension,
            'condition': condition,
            'name': condition.get('name', f"constraint_{len(self.modal_constraints)}"),
            'strength': condition.get('strength', 1.0),
            'enforced_since': datetime.now(timezone.utc).isoformat()
        }
        
        self.modal_constraints.append(constraint)
        
        # Update consistency graph
        self._update_consistency_graph(constraint)
    
    def _update_consistency_graph(self, constraint: Dict):
        """Update modal consistency graph with new constraint"""
        
        # This would build a graph of modal dependencies
        # For now, placeholder implementation
        constraint_id = constraint['name']
        self.consistency_graph.add_node(constraint_id, **constraint)
        
        # Connect to affected entities
        affected_entities = []
        for entity_id, vector in self.current_state.modal_vectors.items():
            if constraint['dimension'] in vector.dimensions:
                affected_entities.append(entity_id)
        
        for entity_id in affected_entities:
            self.consistency_graph.add_edge(constraint_id, entity_id,
                                          weight=constraint['strength'])
    
    def save_state(self, filepath: Path):
        """Save complete world model state to file"""
        
        state = {
            'world_states': [
                {
                    'timestamp': state.timestamp.isoformat(),
                    'modal_vectors': {
                        eid: {
                            'coordinates': vec.coordinates.tolist(),
                            'dimensions': [d.name for d in vec.dimensions],
                            'certainty': vec.certainty.tolist(),
                            'timestamp': vec.timestamp.isoformat(),
                            'source': vec.source,
                            'metadata': vec.metadata
                        }
                        for eid, vec in state.modal_vectors.items()
                    },
                    'relations': state.relations,
                    'global_modality': state.global_modality.tolist(),
                    'consistency_score': state.consistency_score,
                    'entropy': state.entropy
                }
                for state in self.world_states + [self.current_state]
            ],
            'entity_history': {
                eid: [
                    (ts.isoformat(), {
                        'coordinates': vec.coordinates.tolist(),
                        'dimensions': [d.name for d in vec.dimensions],
                        'certainty': vec.certainty.tolist(),
                        'source': vec.source,
                        'metadata': vec.metadata
                    })
                    for ts, vec in history
                ]
                for eid, history in self.entity_history.items()
            },
            'fractal_embeddings': {
                eid: {
                    'base_vector': {
                        'coordinates': emb.base_vector.coordinates.tolist(),
                        'dimensions': [d.name for d in emb.base_vector.dimensions],
                        'certainty': emb.base_vector.certainty.tolist()
                    },
                    'scales': emb.scales,
                    'embeddings': {str(k): v.tolist() for k, v in emb.embeddings.items()},
                    'invariants': emb.invariants,
                    'dimension': emb.dimension
                }
                for eid, emb in self.fractal_embeddings.items()
            },
            'modal_constraints': self.modal_constraints,
            'metrics': self.metrics,
            'metadata': {
                'saved_at': datetime.now(timezone.utc).isoformat(),
                'total_entities': len(self.current_state.modal_vectors),
                'total_states': len(self.world_states) + 1,
                'world_model_version': '1.0.0'
            }
        }
        
        # Compress and save
        compressed = zlib.compress(pickle.dumps(state))
        filepath.parent.mkdir(parents=True, exist_ok=True)
        filepath.write_bytes(compressed)
    
    def load_state(self, filepath: Path):
        """Load world model state from file"""
        
        if not filepath.exists():
            raise FileNotFoundError(f"State file not found: {filepath}")
        
        compressed = filepath.read_bytes()
        state = pickle.loads(zlib.decompress(compressed))
        
        # Clear current state
        self.world_states = []
        self.entity_history = {}
        self.fractal_embeddings = {}
        self.modal_constraints = []
        
        # Load world states
        for state_data in state.get('world_states', []):
            modal_vectors = {}
            for eid, vec_data in state_data['modal_vectors'].items():
                vector = ModalVector(
                    coordinates=np.array(vec_data['coordinates']),
                    dimensions=[getattr(ModalDimension, d) for d in vec_data['dimensions']],
                    certainty=np.array(vec_data['certainty']),
                    timestamp=datetime.fromisoformat(vec_data['timestamp']),
                    source=vec_data['source'],
                    metadata=vec_data.get('metadata', {})
                )
                modal_vectors[eid] = vector
            
            world_state = WorldState(
                timestamp=datetime.fromisoformat(state_data['timestamp']),
                modal_vectors=modal_vectors,
                relations=state_data['relations'],
                global_modality=np.array(state_data['global_modality']),
                consistency_score=state_data['consistency_score'],
                entropy=state_data['entropy']
            )
            
            self.world_states.append(world_state)
        
        # Set current state to most recent
        if self.world_states:
            self.current_state = self.world_states.pop()
        
        # Load entity history
        for eid, history_data in state.get('entity_history', {}).items():
            self.entity_history[eid] = [
                (datetime.fromisoformat(ts), 
                 ModalVector(
                     coordinates=np.array(vec['coordinates']),
                     dimensions=[getattr(ModalDimension, d) for d in vec['dimensions']],
                     certainty=np.array(vec['certainty']),
                     timestamp=datetime.fromisoformat(ts),
                     source=vec['source'],
                     metadata=vec.get('metadata', {})
                 ))
                for ts, vec in history_data
            ]
        
        # Load fractal embeddings
        for eid, emb_data in state.get('fractal_embeddings', {}).items():
            base_vector_data = emb_data['base_vector']
            base_vector = ModalVector(
                coordinates=np.array(base_vector_data['coordinates']),
                dimensions=[getattr(ModalDimension, d) for d in base_vector_data['dimensions']],
                certainty=np.array(base_vector_data['certainty']),
                timestamp=datetime.now(),
                source="loaded_from_state"
            )
            
            embeddings = {float(k): np.array(v) for k, v in emb_data['embeddings'].items()}
            
            self.fractal_embeddings[eid] = FractalEmbedding(
                base_vector=base_vector,
                scales=emb_data['scales'],
                embeddings=embeddings,
                invariants=emb_data['invariants'],
                dimension=emb_data['dimension']
            )
        
        # Load constraints and metrics
        self.modal_constraints = state.get('modal_constraints', [])
        self.metrics.update(state.get('metrics', {}))
    
    def get_diagnostics(self) -> Dict:
        """Get comprehensive system diagnostics"""
        
        return {
            'state': {
                'current_entities': len(self.current_state.modal_vectors),
                'total_history_entries': sum(len(h) for h in self.entity_history.values()),
                'fractal_embeddings': len(self.fractal_embeddings),
                'modal_constraints': len(self.modal_constraints),
                'world_entropy': self.current_state.entropy,
                'consistency_score': self.current_state.consistency_score
            },
            'metrics': self.metrics,
            'modal_distribution': self._get_modal_distribution(),
            'fractal_statistics': self._get_fractal_statistics(),
            'performance': {
                'cache_hit_rate': self.metrics['cache_hits'] / 
                                 max(1, self.metrics['cache_hits'] + self.metrics['cache_misses']),
                'updates_per_second': self.metrics['updates'] / 
                                     max(1, (datetime.now(timezone.utc) - 
                                           self.current_state.timestamp).total_seconds())
            }
        }
    
    def _get_modal_distribution(self) -> Dict:
        """Get distribution of modal values across dimensions"""
        
        distribution = {}
        
        for dim in self.dimensions:
            values = []
            for vector in self.current_state.modal_vectors.values():
                if dim in vector.dimensions:
                    idx = vector.dimensions.index(dim)
                    values.append(vector.coordinates[idx])
            
            if values:
                distribution[dim.name] = {
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'min': float(np.min(values)),
                    'max': float(np.max(values)),
                    'count': len(values)
                }
        
        return distribution
    
    def _get_fractal_statistics(self) -> Dict:
        """Get statistics on fractal embeddings"""
        
        if not self.fractal_embeddings:
            return {}
        
        dimensions = [emb.dimension for emb in self.fractal_embeddings.values()]
        
        return {
            'count': len(self.fractal_embeddings),
            'mean_dimension': float(np.mean(dimensions)),
            'std_dimension': float(np.std(dimensions)),
            'min_dimension': float(np.min(dimensions)),
            'max_dimension': float(np.max(dimensions)),
            'entities_with_fractals': list(self.fractal_embeddings.keys())[:10]
        }

# ==================== PXL INTEGRATION LAYER ====================

class PXLWorldModelBridge:
    """
    Bridge between PXL formal system and modal world model
    Provides bidirectional translation and consistency maintenance
    """
    
    def __init__(self, pxl_system, world_model: DynamicWorldModel):
        self.pxl = pxl_system
        self.world = world_model
        
        # Mapping between PXL constructs and modal dimensions
        self.pxl_to_modal = {
            'ð•€â‚': ModalDimension.PXL_IDENTITY,
            'ð•€â‚‚': ModalDimension.PXL_CONTRADICTION,
            'ð•€â‚ƒ': ModalDimension.PXL_EXCLUDED_MIDDLE,
            'coherence': ModalDimension.COHERENCE
        }
        
        self.modal_to_pxl = {v: k for k, v in self.pxl_to_modal.items()}
        
        # Consistency rules
        self.consistency_rules = self._initialize_consistency_rules()
    
    def pxl_proposition_to_modal(
        self, proposition: str, pxl_analysis: Dict
    ) -> ModalVector:
        """Convert PXL proposition to modal vector."""

        # Placeholder conversion: map known dimensions to zeroed vector
        return ModalVector(
            dimensions=list(self.modal_to_pxl.keys()),
            coordinates={dim: 0.0 for dim in self.modal_to_pxl.keys()},
        )